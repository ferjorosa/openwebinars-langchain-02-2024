{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Introducción a los Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - ¿Qué son los Large Language Models (LLMs)? (10-15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indice\n",
    "\n",
    "* ¿Qué es un modelo de lenguaje?\n",
    "   \n",
    "* Breve historia de los modelos de lenguaje\n",
    "\n",
    "* Tokenización\n",
    "\n",
    "* ¿Cómo se construye un LLM?\n",
    "    * Ejemplo: Proceso de entrenamiento de Chat-GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 - ¿Qué es un modelo de lenguaje?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modelo de lenguaje resuelve el siguiente problema: dado una secuencia de palabras, predecir que palabra vendrá a continuación, y esta predicción se presenta en forma de una distribución de probabilidad.\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/modelo_de_lenguaje.png\" width=\"800\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "En este ejemplo, la secuencia de palabras con la que comenzamos es \"la vaca fue al\", y alimentamos esto al modelo de lenguaje, que produce una predicción en forma de una distribución sobre qué palabra vendrá a continuación. En este caso, el modelo piensa que \"campo\" es lo más probable, seguido posiblemente por \"prado\", \"granero\" y el resto de palabras de su vocabulario (en este caso solo mostramos el top 4). \n",
    "\n",
    "Interesantemente, en la cuarta posición de mayor probabilidad se encuentra la palabra \"cole\", en referente a \"colegio\". Intuitivamente \"cole\" no parece ser una palabra que debiera tener una alta probabilidad de seguir en esta frase. Sin embargo, todo depende de que dato se haya utilizado para entrenar nuestro modelo de lenguaje.\n",
    "\n",
    "Quizás nuestro dato de entrenamiento contenia información acerca de libros, blogs y páginas webs donde se referenciaba al cuento infantil \"La vaca que fue al cole\" y el modelo de lenguaje acabó interiorizando que la palabra \"cole\" podia ser probable. Especialmente en una frase tan corta como \"la vaca fue al\", donde necesitamos mas contexto para orientar al modelo de lenguaje.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/la_vaca_que_fue_al_cole.jpg\" width=\"250\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "¿Cómo usamos un modelo de lenguaje para generar texto, como se ve en un chatbot GPT o algo similar? Es bastante simple. Tomamos nuestra secuencia de palabras \"la vaca fue al\", obtenemos nuestra predicción sobre qué palabra vendrá a continuación, elegimos una palabra al azar de esa distribución y esa es la siguiente palabra que generamos. Agregamos esa palabra a la secuencia inicial y eso se convierte en nuestra nueva secuencia, que alimentamos de nuevo al modelo de lenguaje para obtener una nueva distribución de probabilidad, y así sucesivamente. \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/generation_process.png\" width=\"700\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 - Breve historia de los modelos de lenguaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos de lenguaje no son nuevos, de hecho, tienen al menos 75 años. Claude Shannon habló sobre modelos de lenguaje en su trabajo seminal sobre teoría de la información en 1948, y construyó modelos de lenguaje manualmente examinando libros y estimando las frecuencias relativas de las palabras para construir sus modelos de lenguaje.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/shannon.jpg\" width=\"200\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Entonces, ¿qué ha cambiado en los últimos 75 años? ¿Qué es especial en los modelos de lenguaje modernos que tienen estas capacidades asombrosas? Hay muchas cosas, pero destacaré una que considero muy importante, y es **el tamaño de la secuencia que el modelo puede utilizar para predecir la siguiente palabra**. No solo se trata de predecir una palabra que tenga sentido a nivel de oración, como \"la vaca en el granero\", sino de predecir una palabra que tenga sentido en el contexto de un párrafo, una página o un documento, un contexto amplio. Creo que es una de las principales características que hacen que los modelos modernos sean muy poderosos.\n",
    "\n",
    "La evolución de los modelos de lenguaje a lo largo del tiempo ha sido notable, destacaria tres eventos principales en su desarrollo:\n",
    "\n",
    "1. **Modelos de lenguaje tradicionales (antes de 2010):** Antes del auge de la inteligencia artificial y el aprendizaje profundo, los modelos de lenguaje se basaban en estadísticas de n-gramas y reglas gramaticales. Eran limitados en su capacidad para comprender el contexto y no podían generar texto de manera coherente.\n",
    "   \n",
    "2. **Auge del Aprendizaje Profundo (2010 - 2017):** Se produce un avance significativo con la introducción de modelos de lenguaje basados en redes neuronales. Por ejemplo, las redes neuronales recurrentes (RNN) y las redes neuronales de memoria a largo plazo (LSTM por sus siglas en inglés). Estos modelos son capaces de capturar mejor el contexto y de generar texto más coherente, pero tienen problemas con grandes secuencias de texto.\n",
    "   \n",
    "3. **Arquitectura Transformer y auge de los modelos masivos (después de 2017):** La arquitectura Transformer, desarrollada en 2017, sentó las bases para la revolución de modelos de lenguaje masivos, ya que mejoró la eficiencia y la capacidad de modelado en el procesamiento del lenguaje natural. Los modelos masivos, como GPT, basados en esta arquitectura, aprovecharon su capacidad de atención y modelado de secuencias a gran escala, lo que llevó a avances significativos en la generación y comprensión de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 - Tokenización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entendemos lo que es un modelo de lenguaje y sabemos como generar texto a partir de él. Sin embargo, hay un detalle que he simplificado en su explicación que explicaré ahora.\n",
    "\n",
    "Los modelos de lenguaje modernos en realidad no trabajan directamente con palabras, sino que tratan con secuencias de caracteres o, en estos dias, en realidad incluso una secuencia de bytes que es lo que se predice en lugar de una palabra.\n",
    "\n",
    "**¿Por que tratamos con tokens en lugar de palabras?** Los tokens nos dan la flexibilidad para manejar situaciones que las palabras no permitirían. Por ejemplo, en la siguiente la siguiente figura podemos ver como cambia la tokenización de la palabra \"Español\" cuando introducimos un error tipográfico al sustitutir la \"ñ\" por \"n\". Si estuvieramos limitados a palabras en el diccionario, el modelo de lenguaje no sabría como manejarlo.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/tokenizacion_ejemplo.png\" width=\"500\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Además de trabajar con texto tradicional, podemos querer manejar código o expresiones numéricas. Nuevamente, si estuvieramos limitados unicamente a palabras, esto no seria posible.\n",
    "\n",
    "Mencionar que la tokenización que vemos en la imagen [se corresponde con la del modelo GPT-3](https://platform.openai.com/tokenizer). Cada modelo de lenguaje moderno tiene su propio tokenizador que varia segun la estrategia y dato utilizados para su entrenamiento.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 - Cómo se construye un LLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para construir un modelo masivo de lenguaje se requieren tres ingredientes principales:\n",
    "\n",
    "1. **Datos de entrenamiento a gran escala:** Un modelo masivo de lenguaje se entrena con enormes cantidades de datos de texto. Estos datos pueden ser recopilados de la web, libros, artículos, conversaciones, y otros recursos textuales. Cuantos más datos de alta calidad estén disponibles, mejor será el rendimiento del modelo. El tamaño y la diversidad de los datos son esenciales para que el modelo adquiera una comprensión profunda del lenguaje y sea capaz de generalizar en una amplia gama de tareas.\n",
    "\n",
    "\n",
    "2. **Arquitectura de modelo escalable:**  La arquitectura Transformer es comúnmente utilizada en modelos masivos de lenguaje debido a su capacidad para capturar relaciones a largo plazo en el texto y su capacidad para el procesamiento paralelo, lo que permite el entrenamiento eficiente en hardware especializado, como GPUs o TPUs. Estas arquitecturas son altamente configurables y se pueden adaptar a diversas tareas de procesamiento de lenguaje natural.\n",
    "\n",
    "\n",
    "3. **Gran capacidad de computación:** Entrenar y mantener modelos masivos de lenguaje es computacionalmente muy costoso. Se utilizan clusters de servidores con unidades de procesamiento gráfico (GPU) o unidades de procesamiento tensorial (TPU) de alto rendimiento para realizar el entrenamiento a gran escala. Además, se necesita infraestructura para gestionar y servir los modelos entrenados, ya que su tamaño puede ser sustancial. Por poner un ejemplo, entrenar modelos como GPT o LLaMA cuesta millones de dólares.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5 - Ejemplo: Proceso de entrenamiento de Chat-GPT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/chat_gpt_proceso.png\" width=\"900\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.6 - Siguiente:\n",
    "\n",
    "Como vemos, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Aplicaciones de los LLMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indice\n",
    "\n",
    "* Introducción\n",
    "  \n",
    "* Tareas de texto\n",
    "   \n",
    "* Tareas multimodales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 - Introducción\n",
    "\n",
    "Nota: Dividir en 2 si lo hacemos en forma de slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos comentado anteriormente, los modelos de lenguaje son una clase de modelos probabilísticos que se pueden utilizar para generar texto.\n",
    "\n",
    "Aunque esta premisa parece simple, los modelos de lenguaje se han vuelto cada vez más populares por su capacidad para ser aplicados en una gran variedad de tareas, muchas de las cuales el modelo no había sido específicamente entrenado. A este comportamiento se le denomina \"habilidades emergentes\".\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/palm.gif\" width=\"800\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Dentro de estas tareas, vamos a distinguir dos grupos principales:\n",
    "\n",
    "* **Tareas de texto:** aquellas en las que el modelo recibe texto y genera texto. Son las tareas \"originales\" realizadas por los modelos de lenguaje más comunes.\n",
    "  * Traducción\n",
    "  * Asistentes virtuales\n",
    "  * Generación de código\n",
    "  * Buscador inteligente para documentos o páginas webs\n",
    "  * etc.\n",
    "* **Tareas multimodales:** aquellas en las que el modelo puede recibir no solo texto, sino también imágenes, audio o video. Del mismo modo, existen modelos capaces de generar no solo texto, sino también una o varias de estas modalidades.\n",
    "  * Razonamiento sobre imágenes\n",
    "  * Generación de imágenes a partir de texto\n",
    "  * Generación de vídeos a partir de texto\n",
    "  * Manejo de robots\n",
    "  * etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 - Tareas de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo 1: Traducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/google_translate.png\" width=\"800\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/chat_gpt_traduccion.png\" width=\"800\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo 2: Asistentes virtuales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/asistente_n26.jpg\" width=\"250\"/></td>\n",
    "        <td><img src=\"./images_1/asistente_viajar.avif\" width=\"250\"/></td>\n",
    "        <td><img src=\"./images_1/asistente_her.jpg\" width=\"250\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo 3: Generación de código #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/github_copilot.gif\" width=\"600\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo 3: Generación de código #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/chat_gpt_codigo.png\" width=\"600\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo 4: Buscador inteligente para documentos o páginas webs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/bing_chat_2.png\" width=\"250\"/></td>\n",
    "        <td><img src=\"./images_1/perplexity_documento.png\" width=\"250\"/></td>        \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs generativos basados unicamente en texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **T5** ([Raffel et al., 2019](https://arxiv.org/abs/1910.10683)) [Google]\n",
    "* **GPT-2**([Radford et al., 2019](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)) [OpenAI]\n",
    "* **GPT-3** ([Brown et al., 2020](https://arxiv.org/abs/2005.14165)) [OpenAI]\n",
    "* **PaLM** ([Chowdhery et al., 2022](https://arxiv.org/abs/2204.02311)) [Google]\n",
    "* **Chinchilla** ([Hoffmann et al, 2022](https://arxiv.org/pdf/2203.15556.pdf)) [Google]\n",
    "* **Falcon** ([Penedo et al., 2023](https://arxiv.org/abs/2306.01116)) [TII, Abu Dabi]\n",
    "* **LLaMA** ([Touvron et al., 2023](https://arxiv.org/abs/2302.13971)) [Meta]\n",
    "* **Orca-13B** ([Mukherjee et al., 2023](https://arxiv.org/abs/2306.02707)) [Microsoft]\n",
    "* **MISTRAL-7B** ([Jiang et al., 2023](https://arxiv.org/abs/2310.06825)) [Mistral]\n",
    "* **Zephyr-7B** ([Tunstall et al., 2023](https://arxiv.org/abs/2310.16944)) [HuggingFace]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 - Tareas multimodales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo 1: Razonamiento sobre imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/explicar_imagen.png\" width=\"800\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo 2: Generación de imágenes a partir de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/vaca_escuela_1.png\" width=\"250\"/></td>\n",
    "        <td><img src=\"./images_1/vaca_escuela_2.png\" width=\"250\"/></td>\n",
    "        <td><img src=\"./images_1/vaca_escuela_3.png\" width=\"250\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\"Genera una imagen de una vaca que sabe leer y va al colegio\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo 3: Generación de videos a partir de texto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/text_to_video.gif\" width=\"600\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* \"A dog wearing a superhero cape flying through the sky\"\n",
    "* \"A spaceship landing on Mars\"\n",
    "* \"An artist's brush painting on a canvas close up, highly detailed\"\n",
    "* \"A horse drinking water\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo 4: Manejo de robots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/palm_e.gif\" width=\"600\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "[PaLM-E:](https://arxiv.org/abs/2303.03378) \"Traeme las patatas fritas que están en el cajón\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs generativos multimodales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texto + Imagen\n",
    "* **DALL-E** ([Ramesh et al., 2021](https://arxiv.org/pdf/2102.12092.pdf); [Ramesh et al., 2022](https://cdn.openai.com/papers/dall-e-2.pdf); [Betker et al., 2023](https://cdn.openai.com/papers/dall-e-3.pdf)) [OpenAI]\n",
    "* **Flamingo** ([Alayrac et al., 2022](https://arxiv.org/abs/2204.14198)) [Google]\n",
    "* **Kosmos** ([Huang et al., 2023](https://arxiv.org/abs/2302.14045); [Peng et al., 2023](https://arxiv.org/abs/2306.14824)) [Microsoft]\n",
    "* **GPT-4** ([OpenAI, 2023](https://arxiv.org/abs/2303.08774)) [OpenAI]\n",
    "\n",
    "Texto + Video\n",
    "* **Make-A-Video** ([Singer et al., 2022](https://arxiv.org/abs/2209.14792)) [Meta]\n",
    "\n",
    "Texto + Audio\n",
    "* **TANGO** ([Ghosal et al, 2023](https://declare-lab.net/assets/pdfs/tango-arxiv.pdf)) [DeCLaRe Lab + Singapore UT]\n",
    "* **SeamlessM4T** ([Barrault et al, 2023](https://arxiv.org/abs/2308.11596)) [Meta + UC Berkeley]\n",
    "\n",
    "Texto + Imagen + Video + Audio\n",
    "\n",
    "De momento no hay ningún modelo generativo que trabaje en todas las modalidades, lo más cercano es un espacio de embeddings común:\n",
    "* **ImageBind** ([Girdhar et al., 2023](https://arxiv.org/abs/2305.05665)) [Meta]\n",
    "\n",
    "Robots\n",
    "\n",
    "* **PaLM-E** ([Driess et al., 2023](https://arxiv.org/abs/2303.03378)) [Google]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.X - Siguiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hemos presentado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - LLMs con contexto específico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indice\n",
    "\n",
    "* Maneras de dar contexto a un LLM\n",
    "  * Prompt directo (Zero-shot)\n",
    "  * Prompt con ejemplos (Few-shot)\n",
    "  * Retrieval Augmented Generation (RAG)\n",
    "  * Fine-tuning completo\n",
    "  * Fine-tuning eficiente de parámetros (PEFT)\n",
    "<br></br>\n",
    "* Escogiendo la estrategia de contexto adecuada\n",
    "<br></br>\n",
    "* Herramientas para LLMs con contexto:\n",
    "  * Hugging Face\n",
    "  * Cloud APIs\n",
    "  * LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 - Maneras de dar contexto a un LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La integración precisa del contexto en los modelos de lenguaje es esencial para potenciar su rendimiento en las distintas tareas para las que son utilizados. En este sentido, identificamos cuatro enfoques clave para proporcionar contexto a un modelo de lenguaje:\n",
    "\n",
    "Sin necesidad de re-entrenar el modelo:\n",
    "\n",
    "* **Prompt directo (Zero-shot)**. Consiste en proporcionar instrucciones directas al modelo, indicándole específicamente lo que se espera que realice.\n",
    "\n",
    "* **Prompt con ejemplos (Few-shot)**. Proporciona al modelo ejemplos concretos sobre cómo resolver un problema, solicitando que resuelva problemas similares en el futuro con base en estos ejemplos.\n",
    "\n",
    "* **Retrieval augmented generation (RAG)**. Ofrece al modelo una base de contenido, como varios documentos, sobre los cuales el modelo puede fundamentar su respuesta a un problema determinado.\n",
    "\n",
    "Re-entrenando el modelo:\n",
    "\n",
    "* **Fine-tuning**. Implica la actualización de los pesos del modelo utilizando documentos o ejemplos específicos que se desean incorporar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt directo (Zero-shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La gran mayoria de los LLMs actuales han sido entrenados para seguir instrucciones y por ello son capaces de realizar tareas sin necesidad de entrenamiento.\n",
    "\n",
    "*Prompt*\n",
    "\n",
    "```\n",
    "Clasifica el sentimiento de este texto (Positivo / Negativo): \n",
    "\n",
    "'Este libro es una maravilla. Te atrapa desde la primera página y te hace sentir una conexión profunda con los personajes'.\n",
    "```\n",
    "\n",
    "*Output*\n",
    "\n",
    "```\n",
    "Positivo\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt con ejemplos (Few-shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque los LLMs muestran capacidades notables de sin necesidad de entrenamiento, aún tienen limitaciones en tareas más complejas cuando se utiliza la configuración zero-shot. Por ello, introducir ejemplos en el prompt puede ayudar a orientar al modelo a un mejor rendimiento.\n",
    "\n",
    "Las demostraciones actúan como condicionantes para ejemplos posteriores en los que deseamos que el modelo genere una respuesta.\n",
    "\n",
    "*Input*\n",
    "\n",
    "```\n",
    "Ejemplo positivo: 'Este restaurante ofrece un servicio excepcional y la comida es deliciosa.'\n",
    "Ejemplo negativo: 'El producto que compré en esta tienda online llegó dañado y el servicio al cliente fue decepcionante.'\n",
    "\n",
    "Ahora, clasifica el sentimiento de la siguiente frase (Positivo / Negativo): \n",
    "\n",
    "'El espectáculo fue una experiencia increíble'.\n",
    "```\n",
    "\n",
    "*Output*\n",
    "\n",
    "```\n",
    "Positivo\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval augmented generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los LLMs puede ser entrenados para llevar a cabo tareas comunes como análisis de sentimientos, reconocimientos de entidades, resúmenes, etc. Estas tareas generalmente no requieren conocimientos adicionales y pueden resolverse con la base de conocimiento \"implícita\" en el modelo.\n",
    "\n",
    "Sin embargo, hay ciertas tareas que requieren tener acceso a conocimiento actual o muy específico en el cual el modelo no ha sido entrenado. Por ello, es posible construir un sistema que acceda a fuentes de conocimiento externas para completas las tareas. Esto ayuda a mejorar la fiabilidad de las respuestas generadas y ayuda a mitigar las \"alucinaciones\".\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1_3/rag_example.png\" width=\"800\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning completo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ajustan todos los parámetros del modelo al nuevo conjunto de datos.\n",
    "\n",
    "**Pros:**\n",
    "- A menudo es el método más efectivo para adaptar el modelo.\n",
    "\n",
    "**Contras:**\n",
    "- Requiere muchos recursos computacionales.\n",
    "- Difícil de implementar en modelos masivos (por ejemplo, GPT-4).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Fine-Tuning Eficiente de Parámetros (PEFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se actualiza solo una parte de los parámetros del modelo con los nuevos datos.\n",
    "\n",
    "**Pros:**\n",
    "- Buena técnica para adaptar el modelo.\n",
    "- Mayor flexibilidad que el fine-tuning completo. Permite crear diferentes versiones del modelo para tareas específicas guardando y intercambiando piezas de pesos modificados para cada tarea.\n",
    "- Menos demandante en recursos computacionales que el fine-tuning completo.\n",
    "\n",
    "**Contras:**\n",
    "- Aún requiere recursos computacionales significativos.\n",
    "- Complicado de implementar en modelos masivos (como GPT-4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 - Escogiendo la estrategia de contexto adecuada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Coste\n",
    "* Complejidad de implementación\n",
    "* Eficacia del modelo\n",
    "  * Nivel de adaptación al dominio\n",
    "  * Actualidad de las respuestas\n",
    "  * Transparencia e interpretabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/comparativa_coste.png\" width=\"400\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* **Prompting (Zero-shot & Few-shot):** El método menos costoso, pero puede requerir actualizar el LLM preentrenado.\n",
    "  \n",
    "* **RAG:** Mas costoso que prompting ya que conlleva mas modelos e infraestructura (e.g., base datos vectorial).\n",
    "  \n",
    "* **PEFT:** Requiere reentrenamientos constantes, y éstos son costosos.\n",
    "  \n",
    "* **Full fine-tuning:** Requiere reentrenamientos costosos, y éstos son muy costosos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complejidad de implementación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/comparativa_implementacion.png\" width=\"400\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* **Prompting (Zero-shot & Few-shot):** Baja complejidad, simplemente hay que crear prompts y probarlas.\n",
    "\n",
    "* **RAG:** Más complejo que prompting porque requiere una mayor infraestructura, requiere conocimientos de ingenieria de software\n",
    "\n",
    "* **PEFT & Full Fine-tuning:** Implementarlo correctamente requiere conocimientos avanzados de Deep Learning, NLP y computación paralela con múltiples GPUs. Por ello se suele contratar como servicio a empresas especializadas (e.g., OpenAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eficacia del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medir la \"eficacia\" del LLM es complejo porque la \"eficacia\" a menudo depende de una combinación de métricas distintas.\n",
    "\n",
    "La importancia de estas métricas puede variar según el caso de uso específico. Algunas aplicaciones puede priorizar la jerga específica del dominio. Otras pueden priorizar la capacidad de rastrear la respuesta del modelo hasta una fuente particular.\n",
    "\n",
    "Es por esto que solemos distinguir tres métricas principales para evaluar la \"eficacia\" de un LLM:\n",
    "* Nivel de adaptación al dominio\n",
    "* Actualidad de las respuestas\n",
    "* Transparencia e interpretabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nivel de adaptación al dominio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/comparativa_adaptacion.png\" width=\"400\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Si bien RAG es competente en la recuperación de datos, no está diseñado para recuperar patrones o vocabulario especifico del dominio.\n",
    "\n",
    "Para tareas que buscan una fuerte afinidad con el dominio, PEFT y full fine-tuning son el camino a seguir. Por ejemplo, si queremos que el modelo sepa programar en un lenguaje de programación que nunca ha visto durante su entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actualidad de las respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/comparativa_actualidad.png\" width=\"400\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Las estrategias basadas en fine-tuning deben ser constantement aplicadas si queremos que nuestro LLM esté al dia. En contra, RAG funciona como un interfaz que no se deberia ver afectado por la actualización de los datos, siendo ideal para entornos con datos cambiantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transparencia e interpretabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/comparativa_transparencia.png\" width=\"400\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "En ciertas aplicaciones, comprender el proceso de toma de decisiones del modelo es fundamental. Mientras que el fine-tuning opera más como una \"caja negra\", ocultando su razonamiento, RAG ofrece una visión más clara. Su proceso de dos pasos identifica los documentos recuperados, mejorando la confianza y comprensión del usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 - Herramientas para LLMs con contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a explicar las diferentes maneras con las que podemos trabajar con LLMs, ordenadas de mayor a menor \"flexibilidad\" pero tambien de mayor a menor trabajo necesario para realizar aplicaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/hugging_face.png\" width=\"500\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Hugging Face ofrece en su Hub más de 350k modelos y 75k datasets open-source. \n",
    "\n",
    "La forma más directa de utilizar un LLM. Podemos descargarnos el modelo puro y trabajar con el en PyTorch.\n",
    "\n",
    "Tiene sentido si queremos hace fine-tuning al modelo por nuestra cuenta y contamos con la capacidad computacional necesaria.\n",
    "\n",
    "No tiene sentido utilizar unicamente Hugging Face para hacer aplicaciones como RAG porque tendriamos que implementar toda la infraestructura por nuestra cuenta:\n",
    "* Implementar entrenamiento e inferencia del modelo de forma escalable.\n",
    "* Implementar todos los conectores a bases de datos, internet, Wikipedia, documentos, etc., asi como el código para programar agentes inteligentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cloud APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No todos los modelos son open-source. Muchos de ellos solo se ofrecen en forma de API, ya sea directamente por la propia empresa que ha generado el modelo:\n",
    "* Google (e.g., PALM)\n",
    "* Anthropic (Claude)\n",
    "* OpenAI (GPT y DALL-E)\n",
    "* Mosaic ML (MPT-7B)\n",
    "\n",
    "o mediante un cloud provider como\n",
    "* AWS\n",
    "* Google Cloud\n",
    "* Azure\n",
    "\n",
    "La ventaja de usar una API es que eliminamos la complejidad de implementar entrenamientos o sistemas de inferencia escalables\n",
    "\n",
    "La desventajas son:\n",
    "* Dependiendo de nuestra experiencia puede ser más barato/caro, ya que pagamos a otra empresa para que nos haga parte del trabajo\n",
    "* Seguimos teniendo que implementar conectores y todo el código relacionado con agentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poner un diagram como el de howard con una imagen diagrama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.X - Siguiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto terminariamos el primer bloque sobre modelos de lenguaje. En el siguiente bloque introduciremos LangChain y presentaremos como podemos utilizarlo."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
